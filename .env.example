# SFL Prompt Studio - Multi-Provider AI Configuration
# Configure at least one AI provider for the application to work
# This is the root configuration - specific service configs are in backend/.env and frontend/.env.local

# === PRIMARY PROVIDERS (Fully Supported) ===

# Google Gemini (Recommended - fast and cost-effective)
GOOGLE_AI_API_KEY=your_google_ai_api_key_here
GOOGLE_DEFAULT_MODEL=gemini-2.5-flash

# OpenAI (Industry standard)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_DEFAULT_MODEL=gpt-4o-mini

# Anthropic Claude (Advanced reasoning)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022

# OpenRouter (Gateway to multiple providers)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_DEFAULT_MODEL=meta-llama/llama-3.1-8b-instruct:free

# Groq (Ultra-fast inference)
GROQ_API_KEY=your_groq_api_key_here
GROQ_DEFAULT_MODEL=llama-3.1-8b-instant

# === SELF-HOSTED PROVIDERS ===

# Ollama (Local deployment)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2:latest
OLLAMA_TIMEOUT=30000

# === ADDITIONAL PROVIDERS (Coming Soon) ===

# Cohere (Enterprise-focused)
COHERE_API_KEY=your_cohere_api_key_here
COHERE_DEFAULT_MODEL=command-r-plus

# Mistral AI (Open-source models)
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_DEFAULT_MODEL=mistral-large-latest

# === AI SERVICE CONFIGURATION ===
DEFAULT_AI_PROVIDER=google
FALLBACK_AI_PROVIDER=openai
ENABLE_PROVIDER_FALLBACK=true
PROVIDER_TIMEOUT=30000

# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=sfl_prompt_studio
DB_USER=your_db_user
DB_PASSWORD=your_db_password

# Server Configuration
PORT=3001
NODE_ENV=development

# Redis Configuration (for job queue)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Logging
LOG_LEVEL=info