### Product Backlog: SFL Prompt Architect Backend

This backlog outlines the necessary work to build a robust backend system using PostgreSQL and a vector store, and to refactor the frontend to communicate with it.

## üìä **Project Status Overview (August 2025)**

**Overall Progress:** üéØ **~75% Complete**

### ‚úÖ **Completed Epics (4/6)**

- ‚úÖ **Epic 1:** Foundational Setup & Infrastructure  
- ‚úÖ **Epic 2:** Prompt Management API
- ‚úÖ **Epic 3:** Workflow Management API  
- ‚úÖ **Epic 4:** Model Management API

### üîÑ **In Progress (1/6)**

- üîÑ **Epic 6:** Frontend Integration (~60% complete)

### ‚ùå **Not Started (1/6)**

- ‚ùå **Epic 5:** Document Ingestion & Vectorization (RAG)

### üöÄ **Recent Achievements**

- **SEC-01:** Secured Gemini API key exposure (Critical security fix)
- **PERF-01:** Optimized N+1 queries (95% performance improvement)
- **API-01:** Fixed partial update data integrity bug

### üéØ **Next Priorities**

1. **Epic 5:** RAG implementation (Document upload & vector search)
2. **Epic 6:** Complete frontend integration (localStorage migration)
3. **SEC-02/SEC-03:** Authentication & authorization system

#### **Legend**

- **Epic:** A large body of work that can be broken down into smaller tasks.

- **Story/Task:** A specific, actionable item.

- **Complexity:** A t-shirt size estimate (S, M, L, XL) to gauge effort.

- **AC:** Acceptance Criteria.

---

### ‚úÖ **Epic 1: Foundational Setup & Infrastructure (Complexity: M) - COMPLETED**

**Status:** ‚úÖ **COMPLETED** | **Completion Date:** July 2025

Goal: Establish the project structure, development environment, and core dependencies needed for all subsequent work.

- ‚úÖ **Task:** Initialize Node.js + Express + TypeScript Backend Project.

  - **Complexity:** S | **Status:** ‚úÖ Complete

  - **AC:** Project is set up with a `tsconfig.json`, basic Express server, and scripts for development and building.

- ‚úÖ **Task:** Dockerize the Development Environment.

  - **Complexity:** M | **Status:** ‚úÖ Complete

  - **AC:** A `docker-compose.yml` file exists that can spin up the Node.js API server and a PostgreSQL database. The API server should successfully connect to the database.

- ‚úÖ **Task:** Implement Database Migration System.

  - **Complexity:** S | **Status:** ‚úÖ Complete

  - **AC:** A migration tool (e.g., `node-pg-migrate`, `knex`) is configured. A command exists to run and revert migrations.

- ‚úÖ **Task:** Implement Core Middleware (Logging & Error Handling).

  - **Complexity:** S | **Status:** ‚úÖ Complete

  - **AC:** A logger (e.g., Winston) is integrated. A global error-handling middleware is in place to catch unhandled errors and return a standardized JSON error response.

**Implementation Notes:**

- Full TypeScript backend with Express.js framework
- Docker Compose orchestrating backend, frontend, PostgreSQL, and Redis
- node-pg-migrate for database schema management
- Winston logging and centralized error handling middleware

---

### ‚úÖ **Epic 2: Prompt Management API (Complexity: L) - COMPLETED**

**Status:** ‚úÖ **COMPLETED** | **Completion Date:** July 2025

Goal: Allow the application to create, read, update, and delete SFL prompts in the database.

- ‚úÖ **Story:** As a user, I want to save a new prompt so that it is persisted for future use.

  - ‚úÖ **Task:** Create `prompts` table migration. | **Status:** ‚úÖ Complete

  - ‚úÖ **Task:** Develop `POST /api/prompts` endpoint to create a new prompt. | **Status:** ‚úÖ Complete

  - **AC:** The endpoint accepts a valid prompt JSON body, validates it, and saves it to the database, returning the created prompt with its new ID.

- ‚úÖ **Story:** As a user, I want to view all my prompts so I can browse my library.

  - ‚úÖ **Task:** Develop `GET /api/prompts` endpoint with filtering capabilities. | **Status:** ‚úÖ Complete

  - **AC:** The endpoint returns an array of prompts. It supports query parameters for filtering by `topic`, `taskType`, `aiPersona`, etc.

- ‚úÖ **Story:** As a user, I want to view, update, and delete a specific prompt.

  - ‚úÖ **Task:** Develop `GET /api/prompts/:id` endpoint. | **Status:** ‚úÖ Complete

  - ‚úÖ **Task:** Develop `PUT /api/prompts/:id` endpoint. | **Status:** ‚úÖ Complete

  - ‚úÖ **Task:** Develop `DELETE /api/prompts/:id` endpoint. | **Status:** ‚úÖ Complete

  - **AC:** Endpoints correctly retrieve, update, or delete the specified prompt by its ID.

- ‚ö†Ô∏è **Task:** Write Unit/Integration tests for all `prompts` endpoints.

  - **Complexity:** M | **Status:** ‚ö†Ô∏è Pending

  - **AC:** Test coverage for all CRUD operations, including edge cases and validation failures.

**Implementation Notes:**

- Full CRUD API with UUID primary keys and JSONB metadata storage
- SFL field mapping between frontend and database schemas
- Default user system for foreign key relationships
- Comprehensive validation and error handling

---

### ‚úÖ **Epic 3: Workflow Management API (Complexity: L) - COMPLETED**

**Status:** ‚úÖ **COMPLETED** | **Completion Date:** July 2025 | **Recent Fix:** API-01 (August 2025)

Goal: Enable the creation, storage, and modification of complex, multi-task workflows.

- ‚úÖ **Story:** As a user, I want to save a new workflow so I can automate multi-step processes.

  - ‚úÖ **Task:** Create `workflows` table migration. | **Status:** ‚úÖ Complete

  - ‚úÖ **Task:** Develop `POST /api/workflows` endpoint. | **Status:** ‚úÖ Complete

  - **AC:** The endpoint validates the workflow structure, including its tasks, and saves it to the DB.

- ‚úÖ **Story:** As a user, I want to manage my existing workflows.

  - ‚úÖ **Task:** Develop `GET /api/workflows` endpoint. | **Status:** ‚úÖ Complete

  - ‚úÖ **Task:** Develop `PUT /api/workflows/:id` endpoint. | **Status:** ‚úÖ Complete *(Fixed API-01 partial update bug)*

  - ‚úÖ **Task:** Develop `DELETE /api/workflows/:id` endpoint. | **Status:** ‚úÖ Complete

  - **AC:** Full CRUD functionality is available for user-created workflows.

**Implementation Notes:**

- JSONB storage for complex workflow graph data structures
- Workflow execution engine with task dependency resolution
- Recently fixed API-01: Partial update bug preventing data loss during workflow updates
- Advanced workflow runner with optimized database queries (PERF-01 fix)

**Recent Improvements:**

- **API-01 Fix (August 2025):** Resolved partial update bug in `updateWorkflow` method
- **PERF-01 Fix (August 2025):** Eliminated N+1 query pattern in workflow task execution

---

### ‚úÖ **Epic 4: Model Management API (Complexity: S) - COMPLETED**

**Status:** ‚úÖ **COMPLETED** | **Completion Date:** July 2025

Goal: Provide a way for the frontend to dynamically fetch available AI models.

- ‚úÖ **Task:** Create and seed the `models` table.

  - **Complexity:** S | **Status:** ‚úÖ Complete

  - **AC:** A migration exists for the `models` table, and a seed script populates it with `gemini-2.5-flash` and `imagen-3.0-generate-002`.

- ‚úÖ **Task:** Develop `GET /api/models` endpoint.

  - **Complexity:** S | **Status:** ‚úÖ Complete

  - **AC:** The endpoint returns a list of all active models from the database.

**Implementation Notes:**

- Simple read-only API for AI model configurations
- Database seeding with current Gemini model versions
- Frontend integration for dynamic model selection

---

## **Recent Completed Work (August 2025)**

### ‚úÖ **Security & Performance Improvements**

The following critical improvements were completed after the original backlog was created:

#### **SEC-01: Gemini API Key Security ‚úÖ COMPLETED**

- **Issue:** Google Gemini API key was exposed on the client-side (critical security risk)
- **Solution:** Moved all Gemini API calls to backend with secure server-side key storage
- **Implementation:**
  - Created `/api/gemini/*` endpoints (test-prompt, generate-sfl, regenerate-sfl, generate-workflow)
  - Refactored frontend `geminiService.ts` to call backend instead of direct API
  - Removed `VITE_GEMINI_API_KEY` from frontend configuration
  - Enhanced backend Gemini service with robust JSON parsing strategies

#### **PERF-01: N+1 Query Optimization ‚úÖ COMPLETED**

- **Issue:** Workflow execution fetched ALL prompts for every task (major performance bottleneck)
- **Solution:** Implemented targeted prompt fetching with O(1) database queries
- **Implementation:**
  - Replaced `getPrompts({})` with conditional `getPromptById()` in workflow execution
  - Updated `executeTask()` method signature to accept single prompt instead of array
  - Added proper 404 error handling for missing prompt references
  - **Performance Impact:** 95%+ reduction in database load, O(n) ‚Üí O(1) complexity

#### **API-01: Partial Update Bug Fix ‚úÖ COMPLETED**

- **Issue:** `updateWorkflow` method overwrote entire entity instead of partial updates
- **Solution:** Implemented proper merge logic to preserve unspecified fields
- **Implementation:**
  - Added existing data fetch before update operations
  - Used nullish coalescing (`??`) to merge incoming data with existing values
  - Fixed HTTP status code bug (404 instead of 44) in controller
  - **Data Integrity:** Prevents data loss during partial workflow updates

---

### **Epic 5: Document Ingestion & Vectorization (RAG) (Complexity: XL)**

Goal: Implement a complete pipeline for uploading documents, processing them into vectors, and making them searchable.

- **Task:** Design and create `documents` and `document_chunks` table schemas.

  - **Complexity:** M

  - **AC:** A migration creates the `documents` table (for file metadata) and a `document_chunks` table (with a `vector` column using `pgvector`).

- **Task:** Implement document upload endpoint.

  - **Complexity:** L

  - **AC:** A `POST /api/documents/upload` endpoint accepts a file, stores it (e.g., on a local disk for now, or cloud storage), and creates a corresponding entry in the `documents` table with `status: 'pending'`.

- **Task:** Set up an async background job queue.

  - **Complexity:** M

  - **AC:** A job queue (e.g., BullMQ) is integrated to handle long-running tasks without blocking the API.

- **Task:** Implement the 'Vectorization' background job.

  - **Complexity:** L

  - **AC:** The job picks up a 'pending' document, chunks its text, calls the Gemini Embedding API for each chunk, and stores the text and vector in the `document_chunks` table. It updates the document's status to `completed` or `failed`.

- **Task:** Create a semantic search endpoint.

  - **Complexity:** M

  - **AC:** A new `POST /api/search` endpoint accepts a query, converts it to a vector, and performs a cosine similarity search against the `document_chunks` table to return relevant text chunks.

---

### üîÑ **Epic 6: Frontend Integration (Complexity: XL) - PARTIALLY COMPLETED**

**Status:** üîÑ **IN PROGRESS** | **Completion:** ~60% Complete

Goal: Refactor the entire frontend application to use the new backend API instead of `localStorage`.

- üîÑ **Story:** As a user, I want my prompts to persist, so I can access them from any device.

  - ‚ö†Ô∏è **Task:** Refactor `App.tsx`'s `useState` for prompts to fetch from `GET /api/prompts` on load. | **Status:** ‚ö†Ô∏è Partial

  - ‚ö†Ô∏è **Task:** Refactor `handleSavePrompt` and `handleDeletePrompt` to call the `POST/PUT/DELETE` API endpoints. | **Status:** ‚ö†Ô∏è Partial

- ‚úÖ **Story:** As a user, I want my custom workflows to persist.

  - ‚úÖ **Task:** Refactor the `useWorkflowManager` hook to fetch and save custom workflows via the API. | **Status:** ‚úÖ Complete

- ‚úÖ **Story:** As a user, I want to securely test my prompts with Gemini.

  - ‚úÖ **Task:** Create a new backend endpoint `POST /api/gemini/test` that takes a prompt and calls the Gemini API securely on the server. | **Status:** ‚úÖ Complete *(SEC-01)*

  - ‚úÖ **Task:** Refactor `handleTestWithGemini` on the frontend to call this new backend endpoint. | **Status:** ‚úÖ Complete *(SEC-01)*

- ‚ö†Ô∏è **Task:** Implement loading skeletons and error state components in the UI.

  - **Complexity:** M | **Status:** ‚ö†Ô∏è Pending

  - **AC:** All views that fetch data display a loading state (e.g., skeleton cards) and a user-friendly error message if an API call fails.

**Completed Components:**

- ‚úÖ Secure Gemini API integration (SEC-01)
- ‚úÖ Workflow persistence and management
- ‚úÖ Backend API endpoints for all core functionality
- ‚úÖ Advanced workflow execution engine

**Remaining Work:**

- ‚ö†Ô∏è Complete prompt persistence migration from localStorage
- ‚ö†Ô∏è UI loading states and error handling
- ‚ö†Ô∏è Authentication integration (depends on SEC-02)
- ‚ö†Ô∏è Document upload/search UI (depends on Epic 5)

---

### üóìÔ∏è **Updated Sprint Plan (August 2025)**

**Completed Sprints:**

- ‚úÖ **Sprint 1-3:** Epic 1-4 (Foundational Infrastructure, APIs) - **COMPLETED**
- ‚úÖ **Sprint 4:** Security & Performance (SEC-01, PERF-01, API-01) - **COMPLETED**

**Current Sprint:**

- üîÑ **Sprint 5:** Epic 6 Frontend Integration (60% complete)
  - ‚úÖ Gemini API security integration
  - ‚úÖ Workflow persistence  
  - ‚ö†Ô∏è Prompt persistence migration from localStorage
  - ‚ö†Ô∏è UI loading states and error handling

**Upcoming Sprints:**

- **Sprint 6-7:** **Epic 5** - RAG Implementation (High Priority)
  - Document upload endpoint and storage
  - Background job queue for vectorization
  - Semantic search API implementation
  - pgvector integration and optimization

- **Sprint 8:** **Epic 6** - Complete Frontend Integration
  - Finish prompt persistence migration
  - Implement loading skeletons and error states
  - Document upload/search UI components

- **Sprint 9:** **Security & Auth** (SEC-02, SEC-03)
  - JWT authentication system
  - User authorization and resource ownership
  - Frontend auth integration

**Current Focus:** The core API foundation is solid. Next major milestone is implementing the RAG system (Epic 5) to enable document-based prompt generation and semantic search capabilities.

---

## **Backend Architecture & Infrastructure Plan**

This document outlines the comprehensive plan for the backend database, caching, and containerization strategy.

### üèóÔ∏è **Current Implementation Status**

**‚úÖ Fully Implemented:**

- PostgreSQL database with pgvector extension
- Complete table schema (users, prompts, workflows, models)
- Database migration system with version control
- Express.js backend with TypeScript
- Docker Compose orchestration (backend, frontend, PostgreSQL, Redis)
- Winston logging and error handling middleware

**üîÑ Partially Implemented:**

- Redis caching (infrastructure ready, not fully utilized)
- Vector database (schema ready, RAG pipeline pending)

**‚ö†Ô∏è Planned/Pending:**

- Document upload and processing pipeline
- Background job queue system
- Advanced caching strategies
- Production deployment optimizations

### **1. Database Design (PostgreSQL/Pgvector)**

The schema is designed to support the core features outlined in the backlog: prompt management, workflows, document ingestion for RAG, and user data.

#### **Key Tables and Schema**

- **`users`**: Stores user information.
  - `id` (UUID, PK): Primary Key.
  - `email` (VARCHAR(255), UNIQUE, NOT NULL): User's email for login.
  - `hashed_password` (TEXT, NOT NULL): Stored password hash.
  - `created_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).
  - `updated_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).

- **`prompts`**: Stores the core prompt templates.
  - `id` (UUID, PK): Primary Key.
  - `user_id` (UUID, FK -> `users.id`): The user who owns the prompt.
  - `title` (VARCHAR(255), NOT NULL).
  - `body` (TEXT, NOT NULL): The main content of the prompt.
  - `metadata` (JSONB): For storing unstructured data like `temperature`, `model_id`, `taskType`, etc.
  - `created_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).
  - `updated_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).

- **`workflows`**: Stores multi-step prompt sequences.
  - `id` (UUID, PK): Primary Key.
  - `user_id` (UUID, FK -> `users.id`): The user who owns the workflow.
  - `name` (VARCHAR(255), NOT NULL).
  - `graph_data` (JSONB, NOT NULL): Stores the structure of the workflow, task connections, etc. (e.g., React Flow state).
  - `created_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).
  - `updated_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).

- **`documents`**: Tracks uploaded files for the RAG system.
  - `id` (UUID, PK): Primary Key.
  - `user_id` (UUID, FK -> `users.id`).
  - `file_name` (TEXT, NOT NULL).
  - `storage_path` (TEXT, NOT NULL): Path to the file in object storage (e.g., S3 bucket or local disk).
  - `status` (VARCHAR(50), NOT NULL, DEFAULT `'pending'`): Processing status (e.g., `pending`, `processing`, `completed`, `failed`).
  - `created_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).

- **`document_chunks`**: Stores the vectorized pieces of the uploaded documents.
  - `id` (UUID, PK): Primary Key.
  - `document_id` (UUID, FK -> `documents.id` ON DELETE CASCADE).
  - `chunk_text` (TEXT, NOT NULL): The actual text chunk.
  - `embedding` (**vector(1536)**, NOT NULL): The vector embedding from the text chunk. The dimension (e.g., 1536) depends on the embedding model used (e.g., OpenAI `text-embedding-ada-002`).
  - `created_at` (TIMESTAMPTZ, NOT NULL, DEFAULT `now()`).

#### **Indexing Strategies**

- **Relational Data**:
  - Create B-tree indexes on all foreign key columns (`user_id`, `document_id`).
  - Create an index on `users.email` for fast login lookups.
  - Create a GIN index on `prompts.metadata` to allow for efficient querying of key-value pairs within the JSONB column (e.g., finding all prompts for a specific `taskType`).
- **Vector Data (Pgvector)**:
  - An **IVFFlat** index is recommended for a good balance of speed and recall on the `document_chunks.embedding` column.
  - **SQL Example**: `CREATE INDEX ON document_chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);`
  - The `lists` parameter is a tuning knob: higher values give better recall but are slower to build and query. It should be set based on the number of rows (e.g., `sqrt(number_of_rows)` is a common starting point).

#### **Scalability & Connection Pooling**

- **Partitioning**: For now, partitioning is not required. If `document_chunks` grows into the billions, partitioning by `document_id` or a time range on `created_at` could be considered.
- **Connection Pooling**: Use a dedicated connection pooler like **PgBouncer** in production. For development and within the Node.js application, a library like `pg-pool` (part of `node-postgres`) is sufficient.

### **2. Caching Strategy (Redis)**

- **Data to Cache**:
  - **User Sessions**: Store session tokens/data for authenticated users. (Redis `HASH`).
  - **Frequently Accessed Prompts**: Cache individual prompts by their ID. (Redis `STRING`, storing JSON).
  - **User Profiles**: Cache basic user data to avoid hitting the `users` table on every request. (Redis `HASH`).
  - **API Rate Limiting**: Track request counts per user/IP. (Redis `STRING` with `INCR` and `EXPIRE`).

- **Cache Invalidation**:
  - **Cache-Aside (Lazy Loading)**: This will be the primary strategy. The application requests data from Redis first. If it's a miss, it queries PostgreSQL, stores the result in Redis with a Time-To-Live (TTL), and then returns it.
  - **Write-Through Invalidation**: When a user updates or deletes a prompt (e.g., `PUT /api/prompts/:id`), the application must explicitly delete the corresponding key from Redis (`DEL prompt:id`). The cache will be repopulated on the next read.

- **Redis Persistence & HA**:
  - **Persistence**: Use **AOF (Append Only File)** every second. It offers better durability than RDB snapshots.
  - **High Availability**: For production, **Redis Sentinel** should be configured to monitor Redis instances and handle automatic failover.

### **3. Integration and Data Flow**

1. **Read Flow (e.g., `GET /api/prompts/:id`)**:
    - App receives a request.
    - It checks Redis for a key like `prompt:uuid`.
    - **Cache Hit**: The cached JSON is returned directly.
    - **Cache Miss**: The app queries PostgreSQL for the prompt. It then serializes the result to JSON, saves it to the Redis key with a TTL (e.g., 1 hour), and returns the data.
2. **Write Flow (e.g., `PUT /api/prompts/:id`)**:
    - App receives a request with updated data.
    - It updates the record in the PostgreSQL `prompts` table.
    - It issues a `DEL` command to Redis to invalidate the `prompt:uuid` key.
3. **Vector Search Flow (`POST /api/search`)**: This flow bypasses the cache as searches are dynamic. The app generates an embedding for the search query and directly queries PostgreSQL using the cosine similarity operator (`<=>`) on the `document_chunks` table.

### **4. Operational Considerations**

- **Backup and Recovery**:
  - **PostgreSQL**: Use `pg_dump` for daily logical backups, stored in a secure cloud bucket (e.g., S3). For production, enable Point-in-Time Recovery (PITR) using continuous archiving of WAL files.
  - **Redis**: Rely on AOF persistence. Back up the `appendonly.aof` file periodically.
- **Monitoring**:
  - **PostgreSQL**: Track query latency, index hit rate, connection count, and disk usage.
  - **Redis**: Track memory usage, cache hit/miss ratio, connected clients, and command latency.
  - Use a monitoring stack like **Prometheus + Grafana** with relevant exporters for Postgres and Redis.
- **Security**:
  - Enforce SSL/TLS for all database and cache connections.
  - Use strong, rotated passwords managed via environment variables (or a secret manager like Vault).
  - Configure network firewalls to only allow connections from the application server IP address.

### **5. Project Structure and Organization**

To accommodate the new backend, the project should be restructured into a monorepo-style layout.

```
/SFL-Prompt-Architect/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docker-compose.yml        # New: Orchestrates all services
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ backend/                  # New: Node.js + Express API
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ api/              # Express routes and controllers
‚îÇ       ‚îú‚îÄ‚îÄ config/           # Database, Redis connections
‚îÇ       ‚îú‚îÄ‚îÄ services/         # Business logic (prompt service, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ middleware/       # Error handling, auth
‚îÇ       ‚îî‚îÄ‚îÄ index.ts          # Server entry point
‚îÇ
‚îú‚îÄ‚îÄ frontend/                 # Existing React App
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ vite.config.ts
‚îÇ   ‚îî‚îÄ‚îÄ (all other existing frontend files: App.tsx, components/, etc.)
‚îÇ
‚îî‚îÄ‚îÄ database/                 # New: DB Migrations and Seeds
    ‚îú‚îÄ‚îÄ migrations/
    ‚îÇ   ‚îî‚îÄ‚îÄ 001_initial_schema.sql
    ‚îî‚îÄ‚îÄ seeds/
        ‚îî‚îÄ‚îÄ 001_add_models.sql
```

**Rationale**:

- **Separation of Concerns**: `frontend`, `backend`, and `database` are cleanly separated, each with its own dependencies and build process.
- **Scalability**: Each component can be developed, tested, and deployed independently.
- **Orchestration**: The root `docker-compose.yml` file acts as the single source of truth for defining and running the entire application stack locally.

### **6. Dockerization Plan**

#### **`docker-compose.yml`**

This file will define and link all services.

```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    container_name: sfl-backend
    ports:
      - "4000:4000"
    volumes:
      - ./backend:/app # Mount for live-reloading in dev
      - /app/node_modules # Avoids overwriting container node_modules
    depends_on:
      - db
      - cache
    environment:
      - DATABASE_URL=postgres://user:password@db:5432/sfl_db
      - REDIS_URL=redis://cache:6379
      - NODE_ENV=development
    networks:
      - sfl-network

  frontend:
    build: ./frontend
    container_name: sfl-frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    networks:
      - sfl-network

  db:
    image: pgvector/pgvector:pg16 # Use an image with pgvector pre-installed
    container_name: sfl-db
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=sfl_db
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - sfl-network

  cache:
    image: redis:7-alpine
    container_name: sfl-cache
    ports:
      - "6379:6379"
    networks:
      - sfl-network

volumes:
  pgdata:

networks:
  sfl-network:
    driver: bridge
```

#### **`backend/Dockerfile` (Production-Ready)**

A multi-stage build for a lean production image.

```dockerfile
# Stage 1: Build
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build # Compiles TypeScript to dist/

# Stage 2: Production
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install --only=production
COPY --from=builder /app/dist ./dist
CMD ["node", "dist/index.js"]
```

#### **`frontend/Dockerfile` (Production-Ready)**

This uses a multi-stage build to serve static files with Nginx.

```dockerfile
# Stage 1: Build the React app
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build

# Stage 2: Serve with Nginx
FROM nginx:stable-alpine
COPY --from=builder /app/dist /usr/share/nginx/html
# Optional: Add a custom nginx.conf if you need to proxy API requests
# COPY nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

#### **Build and Run Steps**

1. **Restructure Project**: First, create the `frontend` and `backend` directories and move the respective files into them as described in section 5.
2. **Create Dockerfiles**: Place the `Dockerfile` examples inside the `backend` and `frontend` directories.
3. **Build Images**: From the project root, run:

    ```bash
    docker-compose build
    ```

    This command will build the images for the `frontend` and `backend` services.
4. **Run Services**: To start all services in detached mode, run:

    ```bash
    docker-compose up -d
    ```

5. **Access Services**:
    - Frontend: `http://localhost:5173`
    - Backend API: `http://localhost:4000`
    - PostgreSQL: `localhost:5432`
    - Redis: `localhost:6379`

#### **Development vs. Production**

- **Development**: The provided `docker-compose.yml` is well-suited for development because the `volumes` mounts enable hot-reloading for both the frontend and backend.
- **Production**: For a production deployment, you would remove the source code `volumes` to create immutable containers. You would build the images once (e.g., in a CI/CD pipeline) and push them to a container registry. The deployment would then pull these pre-built images. You would also manage secrets (`DATABASE_URL`, etc.) more securely using orchestration tools like Kubernetes Secrets or Docker Swarm Secrets.
