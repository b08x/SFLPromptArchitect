/**
 * @file aiStreaming.ts
 * @description TypeScript interfaces and types for AI streaming functionality
 * @version 1.0.0
 * @since 0.6.0
 */

import { AIProvider } from '../services/providerService';

/**
 * Base interface for all streaming-related operations
 */
export interface BaseStreamingRequest {
  provider: AIProvider;
  model: string;
  prompt: string;
  systemMessage?: string;
  requestId?: string;
  timestamp?: number;
}

/**
 * Advanced features supported by AI providers
 */
export interface AIAdvancedFeatures {
  /** Structured JSON output with schema validation */\n  structuredOutput?: {\n    enabled: boolean;\n    schema?: Record<string, unknown>;\n    format?: 'json' | 'yaml' | 'xml';\n  };\n  /** Function/tool calling capabilities */\n  functionCalling?: {\n    enabled: boolean;\n    functions?: AIFunction[];\n    parallel?: boolean;\n  };\n  /** Content safety and filtering */\n  safetySettings?: {\n    enabled: boolean;\n    level?: 'low' | 'medium' | 'high' | 'strict';\n    customFilters?: string[];\n  };\n  /** Response caching */\n  caching?: {\n    enabled: boolean;\n    ttl?: number;\n    key?: string;\n  };\n  /** Vision/multimodal capabilities */\n  vision?: {\n    enabled: boolean;\n    imageUrls?: string[];\n    imageBase64?: string[];\n  };\n}\n\n/**\n * Function definition for AI function calling\n */\nexport interface AIFunction {\n  name: string;\n  description: string;\n  parameters: {\n    type: 'object';\n    properties: Record<string, {\n      type: string;\n      description: string;\n      enum?: string[];\n      required?: boolean;\n    }>;\n    required?: string[];\n  };\n  implementation?: (args: Record<string, unknown>) => Promise<unknown> | unknown;\n}\n\n/**\n * Extended streaming request with advanced features\n */\nexport interface AdvancedStreamingRequest extends BaseStreamingRequest {\n  parameters: Record<string, unknown>;\n  features?: AIAdvancedFeatures;\n  metadata?: {\n    userId?: string;\n    sessionId?: string;\n    conversationId?: string;\n    tags?: string[];\n  };\n}\n\n/**\n * Token usage information\n */\nexport interface TokenUsage {\n  promptTokens: number;\n  completionTokens: number;\n  totalTokens: number;\n  cached?: {\n    promptTokens: number;\n    completionTokens: number;\n  };\n}\n\n/**\n * Cost information for AI requests\n */\nexport interface CostInfo {\n  inputCost: number;\n  outputCost: number;\n  totalCost: number;\n  currency: string;\n  breakdown?: {\n    baseTokens: number;\n    premiumFeatures: number;\n    caching: number;\n  };\n}\n\n/**\n * Performance metrics for AI requests\n */\nexport interface PerformanceMetrics {\n  /** Time to first token (TTFT) in milliseconds */\n  timeToFirstToken?: number;\n  /** Tokens per second during generation */\n  tokensPerSecond?: number;\n  /** Total request duration in milliseconds */\n  totalDuration: number;\n  /** Queue time before processing started */\n  queueTime?: number;\n  /** Processing time on provider side */\n  processingTime?: number;\n  /** Network latency */\n  networkLatency?: number;\n}\n\n/**\n * Enhanced streaming chunk with comprehensive metadata\n */\nexport interface EnhancedStreamChunk {\n  /** Text content of the chunk */\n  content: string;\n  /** Whether this is the final chunk */\n  finished: boolean;\n  /** Error information if something went wrong */\n  error?: {\n    message: string;\n    code?: string;\n    type?: 'rate_limit' | 'auth' | 'model' | 'network' | 'unknown';\n    retryable?: boolean;\n  };\n  /** Token usage information */\n  usage?: TokenUsage;\n  /** Cost information */\n  cost?: CostInfo;\n  /** Performance metrics */\n  metrics?: PerformanceMetrics;\n  /** Function call results if applicable */\n  functionCalls?: {\n    name: string;\n    arguments: Record<string, unknown>;\n    result?: unknown;\n    error?: string;\n  }[];\n  /** Provider-specific metadata */\n  providerMetadata?: {\n    modelVersion?: string;\n    region?: string;\n    endpoint?: string;\n    requestId?: string;\n    cached?: boolean;\n  };\n}\n\n/**\n * Configuration for streaming callbacks\n */\nexport interface StreamingCallbacks {\n  /** Called for each content chunk */\n  onChunk?: (chunk: EnhancedStreamChunk) => void;\n  /** Called when streaming starts */\n  onStart?: (metadata: { provider: AIProvider; model: string; requestId: string }) => void;\n  /** Called when streaming completes successfully */\n  onComplete?: (result: {\n    content: string;\n    usage: TokenUsage;\n    cost: CostInfo;\n    metrics: PerformanceMetrics;\n  }) => void;\n  /** Called when an error occurs */\n  onError?: (error: {\n    message: string;\n    code?: string;\n    type: 'rate_limit' | 'auth' | 'model' | 'network' | 'unknown';\n    retryable: boolean;\n  }) => void;\n  /** Called when function calls are made */\n  onFunctionCall?: (call: {\n    name: string;\n    arguments: Record<string, unknown>;\n  }) => Promise<unknown> | unknown;\n  /** Called for progress updates */\n  onProgress?: (progress: {\n    tokensGenerated: number;\n    estimatedTotal?: number;\n    percentComplete?: number;\n  }) => void;\n}\n\n/**\n * Request configuration with advanced options\n */\nexport interface StreamingRequestConfig {\n  /** Request timeout in milliseconds */\n  timeout?: number;\n  /** Maximum number of retries */\n  maxRetries?: number;\n  /** Retry delay in milliseconds */\n  retryDelay?: number;\n  /** Abort signal for request cancellation */\n  signal?: AbortSignal;\n  /** Priority level for request processing */\n  priority?: 'low' | 'normal' | 'high' | 'urgent';\n  /** Enable request deduplication */\n  deduplicate?: boolean;\n  /** Custom headers to include */\n  headers?: Record<string, string>;\n}\n\n/**\n * Complete streaming request with all options\n */\nexport interface FullStreamingRequest extends AdvancedStreamingRequest {\n  config?: StreamingRequestConfig;\n  callbacks?: StreamingCallbacks;\n}\n\n/**\n * Result of a completed streaming request\n */\nexport interface StreamingResult {\n  success: boolean;\n  content?: string;\n  error?: string;\n  requestId: string;\n  usage?: TokenUsage;\n  cost?: CostInfo;\n  metrics?: PerformanceMetrics;\n  metadata: {\n    provider: AIProvider;\n    model: string;\n    timestamp: string;\n    duration: number;\n    cached: boolean;\n  };\n}\n\n/**\n * Batch streaming request for multiple prompts\n */\nexport interface BatchStreamingRequest {\n  requests: FullStreamingRequest[];\n  config?: {\n    maxConcurrency?: number;\n    failFast?: boolean;\n    timeout?: number;\n  };\n}\n\n/**\n * Result of a batch streaming request\n */\nexport interface BatchStreamingResult {\n  results: StreamingResult[];\n  metadata: {\n    totalRequests: number;\n    successfulRequests: number;\n    failedRequests: number;\n    totalDuration: number;\n    averageDuration: number;\n    totalCost: number;\n    totalTokens: number;\n  };\n}\n\n/**\n * Conversation context for multi-turn interactions\n */\nexport interface ConversationContext {\n  id: string;\n  messages: {\n    role: 'system' | 'user' | 'assistant' | 'function';\n    content: string;\n    metadata?: {\n      timestamp: string;\n      tokens: number;\n      cost: number;\n    };\n  }[];\n  provider: AIProvider;\n  model: string;\n  systemMessage?: string;\n  parameters: Record<string, unknown>;\n  totalTokens: number;\n  totalCost: number;\n  createdAt: string;\n  updatedAt: string;\n}\n\n/**\n * Template for reusable AI interactions\n */\nexport interface AITemplate {\n  id: string;\n  name: string;\n  description: string;\n  category: string;\n  provider: AIProvider;\n  model: string;\n  systemMessage: string;\n  promptTemplate: string;\n  parameters: Record<string, unknown>;\n  variables: {\n    name: string;\n    type: 'string' | 'number' | 'boolean' | 'array' | 'object';\n    required: boolean;\n    description: string;\n    default?: unknown;\n  }[];\n  examples?: {\n    input: Record<string, unknown>;\n    output: string;\n    description?: string;\n  }[];\n  tags: string[];\n  version: string;\n  author: string;\n  createdAt: string;\n  updatedAt: string;\n}\n\n/**\n * Usage analytics for AI interactions\n */\nexport interface AIUsageAnalytics {\n  period: {\n    start: string;\n    end: string;\n  };\n  totalRequests: number;\n  totalTokens: number;\n  totalCost: number;\n  averageLatency: number;\n  successRate: number;\n  providerBreakdown: Record<AIProvider, {\n    requests: number;\n    tokens: number;\n    cost: number;\n    avgLatency: number;\n    successRate: number;\n  }>;\n  modelBreakdown: Record<string, {\n    requests: number;\n    tokens: number;\n    cost: number;\n    avgLatency: number;\n    successRate: number;\n  }>;\n  dailyUsage: {\n    date: string;\n    requests: number;\n    tokens: number;\n    cost: number;\n  }[];\n  topErrors: {\n    error: string;\n    count: number;\n    percentage: number;\n  }[];\n}