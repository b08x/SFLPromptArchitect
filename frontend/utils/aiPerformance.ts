/**\n * @file aiPerformance.ts\n * @description Performance optimization utilities for AI service operations\n * @version 1.0.0\n * @since 0.6.0\n */\n\nimport { AIProvider } from '../services/providerService';\nimport { AIGenerationOptions, AIGenerationResult } from '../services/aiService';\n\n/**\n * Cache configuration for AI responses\n */\ninterface CacheConfig {\n  enabled: boolean;\n  maxSize: number;\n  ttl: number; // Time to live in milliseconds\n  strategy: 'lru' | 'lfu' | 'ttl';\n}\n\n/**\n * Request batching configuration\n */\ninterface BatchConfig {\n  enabled: boolean;\n  maxBatchSize: number;\n  batchTimeout: number; // Max wait time before processing batch\n  priority: 'fifo' | 'priority' | 'model';\n}\n\n/**\n * Performance monitoring metrics\n */\ninterface PerformanceMetrics {\n  requestCount: number;\n  totalLatency: number;\n  averageLatency: number;\n  cacheHitRate: number;\n  errorRate: number;\n  throughput: number; // Requests per second\n  providerStats: Record<AIProvider, {\n    requests: number;\n    avgLatency: number;\n    errorRate: number;\n  }>;\n}\n\n/**\n * Cached response entry\n */\ninterface CacheEntry {\n  key: string;\n  result: AIGenerationResult;\n  timestamp: number;\n  accessCount: number;\n  lastAccessed: number;\n  ttl: number;\n}\n\n/**\n * Batched request entry\n */\ninterface BatchedRequest {\n  id: string;\n  provider: AIProvider;\n  model: string;\n  prompt: string;\n  options: AIGenerationOptions;\n  resolve: (result: AIGenerationResult) => void;\n  reject: (error: Error) => void;\n  timestamp: number;\n  priority: number;\n}\n\n/**\n * AI Performance Optimizer\n * Provides caching, request batching, and performance monitoring for AI operations\n */\nexport class AIPerformanceOptimizer {\n  private static instance: AIPerformanceOptimizer;\n  \n  private cache = new Map<string, CacheEntry>();\n  private batchQueue = new Map<string, BatchedRequest[]>();\n  private metrics: PerformanceMetrics = {\n    requestCount: 0,\n    totalLatency: 0,\n    averageLatency: 0,\n    cacheHitRate: 0,\n    errorRate: 0,\n    throughput: 0,\n    providerStats: {} as Record<AIProvider, any>\n  };\n  \n  private config = {\n    cache: {\n      enabled: true,\n      maxSize: 1000,\n      ttl: 5 * 60 * 1000, // 5 minutes\n      strategy: 'lru' as const\n    },\n    batch: {\n      enabled: true,\n      maxBatchSize: 10,\n      batchTimeout: 100, // 100ms\n      priority: 'model' as const\n    }\n  };\n  \n  private batchTimers = new Map<string, NodeJS.Timeout>();\n  private requestStartTimes = new Map<string, number>();\n\n  private constructor() {\n    // Initialize provider stats\n    const providers: AIProvider[] = ['openai', 'anthropic', 'google', 'openrouter', 'ollama', 'cohere', 'mistral', 'groq'];\n    providers.forEach(provider => {\n      this.metrics.providerStats[provider] = {\n        requests: 0,\n        avgLatency: 0,\n        errorRate: 0\n      };\n    });\n    \n    // Start cleanup interval\n    setInterval(() => this.cleanupCache(), 60000); // Cleanup every minute\n  }\n\n  static getInstance(): AIPerformanceOptimizer {\n    if (!AIPerformanceOptimizer.instance) {\n      AIPerformanceOptimizer.instance = new AIPerformanceOptimizer();\n    }\n    return AIPerformanceOptimizer.instance;\n  }\n\n  /**\n   * Configure the optimizer\n   */\n  configure(config: Partial<{ cache: Partial<CacheConfig>; batch: Partial<BatchConfig> }>) {\n    if (config.cache) {\n      this.config.cache = { ...this.config.cache, ...config.cache };\n    }\n    if (config.batch) {\n      this.config.batch = { ...this.config.batch, ...config.batch };\n    }\n  }\n\n  /**\n   * Generate cache key for a request\n   */\n  private generateCacheKey(\n    provider: AIProvider,\n    model: string,\n    prompt: string,\n    options: AIGenerationOptions\n  ): string {\n    // Create a deterministic hash based on request parameters\n    const keyData = {\n      provider,\n      model,\n      prompt: prompt.trim(),\n      // Only include parameters that affect output\n      temperature: options.temperature,\n      maxTokens: options.maxTokens,\n      topP: options.topP,\n      topK: options.topK,\n      systemMessage: options.systemMessage?.trim(),\n      // Exclude streaming and timeout from cache key\n      parameters: Object.fromEntries(\n        Object.entries(options.parameters || {}).filter(([key]) => \n          !['stream', 'timeout'].includes(key)\n        )\n      )\n    };\n    \n    return btoa(JSON.stringify(keyData)).replace(/[+/=]/g, '');\n  }\n\n  /**\n   * Check if response is cached\n   */\n  private getCachedResponse(key: string): AIGenerationResult | null {\n    if (!this.config.cache.enabled) {\n      return null;\n    }\n\n    const entry = this.cache.get(key);\n    if (!entry) {\n      return null;\n    }\n\n    const now = Date.now();\n    \n    // Check TTL\n    if (now - entry.timestamp > entry.ttl) {\n      this.cache.delete(key);\n      return null;\n    }\n\n    // Update access statistics\n    entry.accessCount++;\n    entry.lastAccessed = now;\n    \n    return {\n      ...entry.result,\n      metadata: {\n        ...entry.result.metadata,\n        cached: true\n      }\n    };\n  }\n\n  /**\n   * Cache a response\n   */\n  private cacheResponse(\n    key: string,\n    result: AIGenerationResult,\n    ttl?: number\n  ): void {\n    if (!this.config.cache.enabled) {\n      return;\n    }\n\n    const now = Date.now();\n    const entry: CacheEntry = {\n      key,\n      result: {\n        ...result,\n        metadata: {\n          ...result.metadata,\n          cached: false // Mark as original, not cached\n        }\n      },\n      timestamp: now,\n      accessCount: 1,\n      lastAccessed: now,\n      ttl: ttl || this.config.cache.ttl\n    };\n\n    // Implement cache eviction if at max size\n    if (this.cache.size >= this.config.cache.maxSize) {\n      this.evictCacheEntries();\n    }\n\n    this.cache.set(key, entry);\n  }\n\n  /**\n   * Evict cache entries based on strategy\n   */\n  private evictCacheEntries(): void {\n    const entries = Array.from(this.cache.entries());\n    let toEvict: string[] = [];\n\n    switch (this.config.cache.strategy) {\n      case 'lru':\n        // Remove least recently used\n        entries.sort((a, b) => a[1].lastAccessed - b[1].lastAccessed);\n        toEvict = entries.slice(0, Math.floor(this.config.cache.maxSize * 0.1)).map(([key]) => key);\n        break;\n        \n      case 'lfu':\n        // Remove least frequently used\n        entries.sort((a, b) => a[1].accessCount - b[1].accessCount);\n        toEvict = entries.slice(0, Math.floor(this.config.cache.maxSize * 0.1)).map(([key]) => key);\n        break;\n        \n      case 'ttl':\n        // Remove expired entries\n        const now = Date.now();\n        toEvict = entries\n          .filter(([, entry]) => now - entry.timestamp > entry.ttl)\n          .map(([key]) => key);\n        break;\n    }\n\n    toEvict.forEach(key => this.cache.delete(key));\n  }\n\n  /**\n   * Clean up expired cache entries\n   */\n  private cleanupCache(): void {\n    const now = Date.now();\n    const expired: string[] = [];\n\n    this.cache.forEach((entry, key) => {\n      if (now - entry.timestamp > entry.ttl) {\n        expired.push(key);\n      }\n    });\n\n    expired.forEach(key => this.cache.delete(key));\n  }\n\n  /**\n   * Generate batch key for similar requests\n   */\n  private generateBatchKey(provider: AIProvider, model: string): string {\n    return `${provider}-${model}`;\n  }\n\n  /**\n   * Add request to batch queue\n   */\n  private addToBatch(\n    batchKey: string,\n    request: BatchedRequest\n  ): void {\n    if (!this.batchQueue.has(batchKey)) {\n      this.batchQueue.set(batchKey, []);\n    }\n\n    const queue = this.batchQueue.get(batchKey)!;\n    queue.push(request);\n\n    // Process batch if it reaches max size\n    if (queue.length >= this.config.batch.maxBatchSize) {\n      this.processBatch(batchKey);\n    } else {\n      // Set timer to process batch after timeout\n      if (this.batchTimers.has(batchKey)) {\n        clearTimeout(this.batchTimers.get(batchKey)!);\n      }\n      \n      const timer = setTimeout(() => {\n        this.processBatch(batchKey);\n      }, this.config.batch.batchTimeout);\n      \n      this.batchTimers.set(batchKey, timer);\n    }\n  }\n\n  /**\n   * Process a batch of requests\n   */\n  private async processBatch(batchKey: string): Promise<void> {\n    const queue = this.batchQueue.get(batchKey);\n    if (!queue || queue.length === 0) {\n      return;\n    }\n\n    // Remove batch from queue\n    this.batchQueue.delete(batchKey);\n    \n    // Clear timer\n    const timer = this.batchTimers.get(batchKey);\n    if (timer) {\n      clearTimeout(timer);\n      this.batchTimers.delete(batchKey);\n    }\n\n    // Sort by priority if needed\n    if (this.config.batch.priority === 'priority') {\n      queue.sort((a, b) => b.priority - a.priority);\n    } else if (this.config.batch.priority === 'model') {\n      queue.sort((a, b) => a.model.localeCompare(b.model));\n    }\n\n    // Process each request individually for now\n    // In a more advanced implementation, you could batch similar requests\n    for (const request of queue) {\n      try {\n        // Import aiService dynamically to avoid circular dependency\n        const { aiService } = await import('../services/aiService');\n        const result = await aiService.generateResponse(\n          request.provider,\n          request.model,\n          request.prompt,\n          request.options\n        );\n        request.resolve(result);\n      } catch (error) {\n        request.reject(error instanceof Error ? error : new Error(String(error)));\n      }\n    }\n  }\n\n  /**\n   * Record request start time\n   */\n  startRequest(requestId: string): void {\n    this.requestStartTimes.set(requestId, Date.now());\n  }\n\n  /**\n   * Record request completion and update metrics\n   */\n  completeRequest(\n    requestId: string,\n    provider: AIProvider,\n    success: boolean,\n    cached: boolean = false\n  ): void {\n    const startTime = this.requestStartTimes.get(requestId);\n    if (startTime) {\n      const latency = Date.now() - startTime;\n      this.updateMetrics(provider, latency, success, cached);\n      this.requestStartTimes.delete(requestId);\n    }\n  }\n\n  /**\n   * Update performance metrics\n   */\n  private updateMetrics(\n    provider: AIProvider,\n    latency: number,\n    success: boolean,\n    cached: boolean\n  ): void {\n    this.metrics.requestCount++;\n    \n    if (!cached) {\n      this.metrics.totalLatency += latency;\n      this.metrics.averageLatency = this.metrics.totalLatency / this.metrics.requestCount;\n    }\n    \n    if (!success) {\n      this.metrics.errorRate = ((this.metrics.errorRate * (this.metrics.requestCount - 1)) + 1) / this.metrics.requestCount;\n    } else {\n      this.metrics.errorRate = (this.metrics.errorRate * (this.metrics.requestCount - 1)) / this.metrics.requestCount;\n    }\n\n    // Update provider stats\n    const providerStats = this.metrics.providerStats[provider];\n    providerStats.requests++;\n    \n    if (!cached) {\n      providerStats.avgLatency = ((providerStats.avgLatency * (providerStats.requests - 1)) + latency) / providerStats.requests;\n    }\n    \n    if (!success) {\n      providerStats.errorRate = ((providerStats.errorRate * (providerStats.requests - 1)) + 1) / providerStats.requests;\n    } else {\n      providerStats.errorRate = (providerStats.errorRate * (providerStats.requests - 1)) / providerStats.requests;\n    }\n    \n    // Update cache hit rate\n    const cacheHits = Array.from(this.cache.values()).reduce((sum, entry) => sum + entry.accessCount, 0);\n    this.metrics.cacheHitRate = cacheHits / this.metrics.requestCount;\n  }\n\n  /**\n   * Optimized request handler with caching and batching\n   */\n  async optimizedRequest(\n    provider: AIProvider,\n    model: string,\n    prompt: string,\n    options: AIGenerationOptions = {}\n  ): Promise<AIGenerationResult> {\n    const requestId = `req-${Date.now()}-${Math.random().toString(36).substring(2)}`;\n    \n    // Start tracking request\n    this.startRequest(requestId);\n    \n    try {\n      // Check cache first\n      const cacheKey = this.generateCacheKey(provider, model, prompt, options);\n      const cachedResult = this.getCachedResponse(cacheKey);\n      \n      if (cachedResult) {\n        this.completeRequest(requestId, provider, true, true);\n        return cachedResult;\n      }\n\n      // Decide whether to batch or execute immediately\n      let result: AIGenerationResult;\n      \n      if (this.config.batch.enabled && !options.stream) {\n        // Add to batch queue\n        result = await new Promise<AIGenerationResult>((resolve, reject) => {\n          const batchKey = this.generateBatchKey(provider, model);\n          const batchedRequest: BatchedRequest = {\n            id: requestId,\n            provider,\n            model,\n            prompt,\n            options,\n            resolve,\n            reject,\n            timestamp: Date.now(),\n            priority: options.priority || 5 // Default priority\n          };\n          \n          this.addToBatch(batchKey, batchedRequest);\n        });\n      } else {\n        // Execute immediately\n        const { aiService } = await import('../services/aiService');\n        result = await aiService.generateResponse(provider, model, prompt, options);\n      }\n\n      // Cache successful results\n      if (result.success && result.response) {\n        this.cacheResponse(cacheKey, result);\n      }\n\n      this.completeRequest(requestId, provider, result.success);\n      return result;\n    } catch (error) {\n      this.completeRequest(requestId, provider, false);\n      throw error;\n    }\n  }\n\n  /**\n   * Get current performance metrics\n   */\n  getMetrics(): PerformanceMetrics {\n    return { ...this.metrics };\n  }\n\n  /**\n   * Get cache statistics\n   */\n  getCacheStats() {\n    const entries = Array.from(this.cache.values());\n    return {\n      size: this.cache.size,\n      maxSize: this.config.cache.maxSize,\n      hitRate: this.metrics.cacheHitRate,\n      totalAccesses: entries.reduce((sum, entry) => sum + entry.accessCount, 0),\n      averageAge: entries.length > 0 \n        ? entries.reduce((sum, entry) => sum + (Date.now() - entry.timestamp), 0) / entries.length / 1000\n        : 0\n    };\n  }\n\n  /**\n   * Clear cache\n   */\n  clearCache(): void {\n    this.cache.clear();\n  }\n\n  /**\n   * Reset metrics\n   */\n  resetMetrics(): void {\n    this.metrics = {\n      requestCount: 0,\n      totalLatency: 0,\n      averageLatency: 0,\n      cacheHitRate: 0,\n      errorRate: 0,\n      throughput: 0,\n      providerStats: {} as Record<AIProvider, any>\n    };\n    \n    const providers: AIProvider[] = ['openai', 'anthropic', 'google', 'openrouter', 'ollama', 'cohere', 'mistral', 'groq'];\n    providers.forEach(provider => {\n      this.metrics.providerStats[provider] = {\n        requests: 0,\n        avgLatency: 0,\n        errorRate: 0\n      };\n    });\n  }\n}\n\n// Export singleton instance\nexport const aiPerformanceOptimizer = AIPerformanceOptimizer.getInstance();\n\n// Export utility functions\nexport function withPerformanceOptimization<T extends (...args: any[]) => Promise<AIGenerationResult>>(\n  fn: T\n): T {\n  return (async (...args: Parameters<T>) => {\n    // This is a decorator pattern for adding performance optimization to any AI function\n    // In practice, you'd need to extract provider/model info from args\n    return await fn(...args);\n  }) as T;\n}\n\n/**\n * Hook for accessing performance metrics in React components\n */\nexport function useAIPerformanceMetrics() {\n  const [metrics, setMetrics] = React.useState<PerformanceMetrics>(aiPerformanceOptimizer.getMetrics());\n  \n  React.useEffect(() => {\n    const interval = setInterval(() => {\n      setMetrics(aiPerformanceOptimizer.getMetrics());\n    }, 1000);\n    \n    return () => clearInterval(interval);\n  }, []);\n  \n  return metrics;\n}\n\n// Note: React import would need to be added to package.json for the hook above\n// For now, it's commented out to avoid import errors\n// import React from 'react';